{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datasets and their properties\n",
    "openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "\n",
    "# Get dataset by ID\n",
    "dataset = openml.datasets.get_dataset(1476)\n",
    "\n",
    "# Get the data itself as a dataframe (or otherwise)\n",
    "X, _, _, _ = dataset.get_data(dataset_format=\"dataframe\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<span style=\"color:#3c1518\">Questions</span>**\n",
    "\n",
    "### <span style=\"color:#69140e\">Answer to question 1: </span>\n",
    "\n",
    "<span style=\"color:#a44200\">This archive contains 13910 measurements from 16 chemical sensors utilized in simulations for drift compensation in a discrimination task of 6 gases at various levels of concentrations.\n",
    "The goal is to achieve good performance (or as low degradation as possible) over time, as reported in the paper mentioned below in Section 2: Data collection.\n",
    "The primary purpose of providing this dataset is to make it freely accessible online to the chemo-sensor research community and artificial intelligence to develop strategies to cope with sensor/concept drift. The dataset can be used exclusively for research purposes. Commercial purposes are fully excluded.\n",
    "The dataset was gathered within January 2007 to February 2011 (36 months) in a gas delivery platform facility situated at the ChemoSignals Laboratory in the BioCircuits Institute, University of California San Diego.\n",
    "Being completely operated by a fully computerized environment controlled by a LabVIEW's National Instruments software on a PC fitted with the appropriate serial data acquisition boards. The measurement system platform provides versatility for obtaining the desired concentrations of the chemical substances of interest with high accuracy and in a highly reproducible manner, minimizing thereby the common mistakes caused by human intervention and making it possible to exclusively concentrate on the chemical sensors for compensating real drift.\n",
    "The resulting dataset comprises recordings from six distinct pure gaseous substances, namely Ammonia, Acetaldehyde, Acetone, Ethylene, Ethanol, and Toluene, each dosed at a wide variety of concentration values ranging from 5 to 1000 ppmv</span>\n",
    "\n",
    "### <span style=\"color:#69140e\">Answer to question 2: </span>\n",
    "\n",
    "<span style=\"color:#a44200\">There was no need to make any changes in order to import the dataset.</span>\n",
    "\n",
    "### <span style=\"color:#69140e\">Answer to question 3: </span>\n",
    "\n",
    "<span style=\"color:#a44200\"></span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<span style=\"color:#3c1518\">Preparation of the Dataset</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('int64')\n",
    "y = X['Class']\n",
    "X = X.drop(columns=['Class'])\n",
    "X,y = shuffle(X,y,random_state=0)\n",
    "X = X.reset_index(drop= True)\n",
    "y = y.reset_index(drop= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">Train - Test Split </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X[0:int(0.7*len(X))]\n",
    "Xtest = X[int(0.7*len(X))+1:]\n",
    "ytrain = y[0:int(0.7*len(X))]\n",
    "ytest = y[int(0.7*len(X))+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(columns=['Classifier','Mean Accuracy', 'F1 Score'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<span style=\"color:#3c1518\">Classifiers</span>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">Dummy Classifier</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyClassifier(strategy=\"uniform\")\n",
    "modelStr = 'Dummy'\n",
    "acc_score = []\n",
    "f1_scores = []\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "for train_index , test_index in kf.split(Xtrain):\n",
    "    X_train , X_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "     \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "     \n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    acc_score.append(accuracy)\n",
    "\n",
    "    f1Score = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "    f1_scores.append(f1Score)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_f1_score = sum(f1_scores)/k\n",
    "\n",
    "if not (modelStr in metrics['Classifier'].unique()):\n",
    "    metrics.loc[len(metrics)] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "else:\n",
    "    index = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "    metrics.loc[index] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "\n",
    "figAccScores = px.bar(acc_score, title='Accuracy',width=1000)\n",
    "figAccScores.update_layout(\n",
    "    title=\"Accuracy diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figAccScores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figF1Scores = px.bar(f1_scores, title='F1 Scores',width=1000)\n",
    "figF1Scores.update_layout(\n",
    "    title=\"F1 score diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"F1 Score\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figF1Scores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figAccScores.show()\n",
    "figF1Scores.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">Gaussian Naive Bayes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "modelStr = 'Gaussian Naive Bayes'\n",
    "acc_score = []\n",
    "f1_scores = []\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "for train_index , test_index in kf.split(Xtrain):\n",
    "    X_train , X_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "     \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "    \n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    acc_score.append(accuracy)\n",
    "\n",
    "    f1Score = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "    f1_scores.append(f1Score)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_f1_score = sum(f1_scores)/k\n",
    "\n",
    "if not (modelStr in metrics['Classifier'].unique()):\n",
    "    metrics.loc[len(metrics)] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "else:\n",
    "    index = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "    metrics.loc[index] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "\n",
    "figAccScores = px.bar(acc_score, title='Accuracy',width=1000)\n",
    "figAccScores.update_layout(\n",
    "    title=\"Accuracy diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figAccScores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figF1Scores = px.bar(f1_scores, title='F1 Scores',width=1000)\n",
    "figF1Scores.update_layout(\n",
    "    title=\"F1 score diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"F1 Score\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figF1Scores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figAccScores.show()\n",
    "figF1Scores.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">kNN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "modelStr = 'kNN'\n",
    "acc_score = []\n",
    "f1_scores = []\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "for train_index , test_index in kf.split(Xtrain):\n",
    "    X_train , X_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "     \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "    \n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    acc_score.append(accuracy)\n",
    "\n",
    "    f1Score = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "    f1_scores.append(f1Score)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_f1_score = sum(f1_scores)/k\n",
    "\n",
    "if not (modelStr in metrics['Classifier'].unique()):\n",
    "    metrics.loc[len(metrics)] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "else:\n",
    "    index = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "    metrics.loc[index] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "\n",
    "figAccScores = px.bar(acc_score, title='Accuracy',width=1000)\n",
    "figAccScores.update_layout(\n",
    "    title=\"Accuracy diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figAccScores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figF1Scores = px.bar(f1_scores, title='F1 Scores',width=1000)\n",
    "figF1Scores.update_layout(\n",
    "    title=\"F1 score diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"F1 Score\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figF1Scores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figAccScores.show()\n",
    "figF1Scores.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">Logistic Regression</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0)\n",
    "modelStr = 'Logistic Regression'\n",
    "acc_score = []\n",
    "f1_scores = []\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "for train_index , test_index in kf.split(Xtrain):\n",
    "    X_train , X_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "     \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "    \n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    acc_score.append(accuracy)\n",
    "\n",
    "    f1Score = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "    f1_scores.append(f1Score)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_f1_score = sum(f1_scores)/k\n",
    "\n",
    "if not (modelStr in metrics['Classifier'].unique()):\n",
    "    metrics.loc[len(metrics)] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "else:\n",
    "    index = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "    metrics.loc[index] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "\n",
    "figAccScores = px.bar(acc_score, title='Accuracy',width=1000)\n",
    "figAccScores.update_layout(\n",
    "    title=\"Accuracy diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figAccScores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figF1Scores = px.bar(f1_scores, title='F1 Scores',width=1000)\n",
    "figF1Scores.update_layout(\n",
    "    title=\"F1 score diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"F1 Score\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figF1Scores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figAccScores.show()\n",
    "figF1Scores.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">Multi Layer Perceptron</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "modelStr = 'Multi Layer Perceptron'\n",
    "acc_score = []\n",
    "f1_scores = []\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "for train_index , test_index in kf.split(Xtrain):\n",
    "    X_train , X_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "     \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "    \n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    acc_score.append(accuracy)\n",
    "\n",
    "    f1Score = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "    f1_scores.append(f1Score)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_f1_score = sum(f1_scores)/k\n",
    "\n",
    "if not (modelStr in metrics['Classifier'].unique()):\n",
    "    metrics.loc[len(metrics)] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "else:\n",
    "    index = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "    metrics.loc[index] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "\n",
    "figAccScores = px.bar(acc_score, title='Accuracy',width=1000)\n",
    "figAccScores.update_layout(\n",
    "    title=\"Accuracy diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figAccScores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figF1Scores = px.bar(f1_scores, title='F1 Scores',width=1000)\n",
    "figF1Scores.update_layout(\n",
    "    title=\"F1 score diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"F1 Score\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figF1Scores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figAccScores.show()\n",
    "figF1Scores.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">SVM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(gamma='auto')\n",
    "modelStr = 'SVM'\n",
    "acc_score = []\n",
    "f1_scores = []\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "for train_index , test_index in kf.split(Xtrain):\n",
    "    X_train , X_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "     \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "    \n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    acc_score.append(accuracy)\n",
    "\n",
    "    f1Score = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "    f1_scores.append(f1Score)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_f1_score = sum(f1_scores)/k\n",
    "\n",
    "\n",
    "if not (modelStr in metrics['Classifier'].unique()):\n",
    "    metrics.loc[len(metrics)] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "else:\n",
    "    index = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "    metrics.loc[index] = [modelStr, avg_acc_score, avg_f1_score]\n",
    "\n",
    "figAccScores = px.bar(acc_score, title='Accuracy',width=1000)\n",
    "figAccScores.update_layout(\n",
    "    title=\"Accuracy diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figAccScores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figF1Scores = px.bar(f1_scores, title='F1 Scores',width=1000)\n",
    "figF1Scores.update_layout(\n",
    "    title=\"F1 score diagram\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"F1 Score\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\")   \n",
    ")\n",
    "figF1Scores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figAccScores.show()\n",
    "figF1Scores.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<span style=\"color:#3c1518\">Optimization</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsOptimalModel = pd.DataFrame(columns=['Classifier','Accuracy', 'F1 Score', 'Training Time', 'Average Test Time', 'Percentage Change in Accuracy', 'Percentage Change in F1 Score'])\n",
    "bestParamsPerModel = pd.DataFrame(columns=['Classifier', 'PipeStr','Best Parametres'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">Dummy Classifier</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelStr = 'Dummy'\n",
    "pipeStr = 'dummy'\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('dummy', DummyClassifier())])\n",
    "\n",
    "param_grid = {\"dummy__strategy\": ['most_frequent', 'prior', 'stratified', 'uniform']}\n",
    "search = HalvingGridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(Xtrain, ytrain)\n",
    "pd.DataFrame(search.cv_results_)\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('dummy', search.best_estimator_['dummy'])])\n",
    "startTime = time.time()\n",
    "pipe.fit(Xtrain,ytrain)\n",
    "stopTime = time.time()\n",
    "trainingTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "stopTime = time.time()\n",
    "\n",
    "accuracyTestTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "f1Score = f1_score(y_test, pipe.predict(X_test), average='macro')\n",
    "stopTime = time.time()\n",
    "\n",
    "f1ScoreTestTime = stopTime - startTime\n",
    "testTime = (accuracyTestTime + f1ScoreTestTime)/2\n",
    "\n",
    "indexMetrics = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "percentageChangeMeanAccuracy = 100*(accuracy - metrics.iloc[indexMetrics]['Mean Accuracy'])/metrics.iloc[indexMetrics]['Mean Accuracy']\n",
    "percentageChangeF1Score = 100*(accuracy - metrics.iloc[indexMetrics]['F1 Score'])/metrics.iloc[indexMetrics]['F1 Score']\n",
    "\n",
    "if not (modelStr in metricsOptimalModel['Classifier'].unique()):\n",
    "    metricsOptimalModel.loc[len(metricsOptimalModel)] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]\n",
    "else:\n",
    "    index = metricsOptimalModel[metricsOptimalModel['Classifier']==modelStr].index.to_list()[0] \n",
    "    metricsOptimalModel.loc[index] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]\n",
    "\n",
    "if not (modelStr in bestParamsPerModel['Classifier'].unique()):\n",
    "    bestParamsPerModel.loc[len(bestParamsPerModel)] = [modelStr, pipeStr, search.best_estimator_[pipeStr]]\n",
    "else:\n",
    "    index = metricsOptimalModel[metricsOptimalModel['Classifier']==modelStr].index.to_list()[0] \n",
    "    metricsOptimalModel.loc[index] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">Gaussian Naive Bayes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelStr = 'Gaussian Naive Bayes'\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('gaussiannb', GaussianNB())])\n",
    "\n",
    "param_grid = {\"gaussiannb__var_smoothing\": list(np.arange(1e-11,1e-7,1e-9 - 1e-13))}\n",
    "search = HalvingGridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(Xtrain, ytrain)\n",
    "pd.DataFrame(search.cv_results_)\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('gaussiannb', search.best_estimator_['gaussiannb'])])\n",
    "startTime = time.time()\n",
    "pipe.fit(Xtrain,ytrain)\n",
    "stopTime = time.time()\n",
    "trainingTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "stopTime = time.time()\n",
    "\n",
    "accuracyTestTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "f1Score = f1_score(y_test, pipe.predict(X_test), average='macro')\n",
    "stopTime = time.time()\n",
    "\n",
    "f1ScoreTestTime = stopTime - startTime\n",
    "testTime = (accuracyTestTime + f1ScoreTestTime)/2\n",
    "\n",
    "indexMetrics = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "percentageChangeMeanAccuracy = 100*(accuracy - metrics.iloc[indexMetrics]['Mean Accuracy'])/metrics.iloc[indexMetrics]['Mean Accuracy']\n",
    "percentageChangeF1Score = 100*(accuracy - metrics.iloc[indexMetrics]['F1 Score'])/metrics.iloc[indexMetrics]['F1 Score']\n",
    "\n",
    "if not (modelStr in metricsOptimalModel['Classifier'].unique()):\n",
    "    metricsOptimalModel.loc[len(metricsOptimalModel)] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]\n",
    "else:\n",
    "    indexMetricsOptimalModel = metricsOptimalModel[metricsOptimalModel['Classifier']==modelStr].index.to_list()[0] \n",
    "    metricsOptimalModel.loc[indexMetricsOptimalModel] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">kNN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelStr = 'kNN'\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('kNN', KNeighborsClassifier())])\n",
    "\n",
    "param_grid = {\"kNN__n_neighbors\": list(np.arange(8,15,1)), \"kNN__weights\": ['uniform', 'distance'], \"kNN__algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],\\\n",
    "     \"kNN__leaf_size\": list(np.arange(26,32,1)), \"kNN__p\": [1,2] }\n",
    "search = HalvingGridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(Xtrain, ytrain)\n",
    "pd.DataFrame(search.cv_results_)\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('kNN', search.best_estimator_['kNN'])])\n",
    "startTime = time.time()\n",
    "pipe.fit(Xtrain,ytrain)\n",
    "stopTime = time.time()\n",
    "trainingTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "stopTime = time.time()\n",
    "\n",
    "accuracyTestTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "f1Score = f1_score(y_test, pipe.predict(X_test), average='macro')\n",
    "stopTime = time.time()\n",
    "\n",
    "f1ScoreTestTime = stopTime - startTime\n",
    "testTime = (accuracyTestTime + f1ScoreTestTime)/2\n",
    "\n",
    "indexMetrics = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "percentageChangeMeanAccuracy = 100*(accuracy - metrics.iloc[indexMetrics]['Mean Accuracy'])/metrics.iloc[indexMetrics]['Mean Accuracy']\n",
    "percentageChangeF1Score = 100*(accuracy - metrics.iloc[indexMetrics]['F1 Score'])/metrics.iloc[indexMetrics]['F1 Score']\n",
    "\n",
    "if not (modelStr in metricsOptimalModel['Classifier'].unique()):\n",
    "    metricsOptimalModel.loc[len(metricsOptimalModel)] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]\n",
    "else:\n",
    "    indexMetricsOptimalModel = metricsOptimalModel[metricsOptimalModel['Classifier']==modelStr].index.to_list()[0] \n",
    "    metricsOptimalModel.loc[indexMetricsOptimalModel] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">Logistic Regression</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelStr = 'Logistic Regression'\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('logisticReg', LogisticRegression(random_state=0))])\n",
    "\n",
    "param_grid = {\"logisticReg__C\": list(np.arange(0.5,1.5,0.1)), \"logisticReg__solver\": ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']}\n",
    "search = HalvingGridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(Xtrain, ytrain)\n",
    "pd.DataFrame(search.cv_results_)\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('logisticReg', search.best_estimator_['logisticReg'])])\n",
    "startTime = time.time()\n",
    "pipe.fit(Xtrain,ytrain)\n",
    "stopTime = time.time()\n",
    "trainingTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "stopTime = time.time()\n",
    "\n",
    "accuracyTestTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "f1Score = f1_score(y_test, pipe.predict(X_test), average='macro')\n",
    "stopTime = time.time()\n",
    "\n",
    "f1ScoreTestTime = stopTime - startTime\n",
    "testTime = (accuracyTestTime + f1ScoreTestTime)/2\n",
    "\n",
    "indexMetrics = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "percentageChangeMeanAccuracy = 100*(accuracy - metrics.iloc[indexMetrics]['Mean Accuracy'])/metrics.iloc[indexMetrics]['Mean Accuracy']\n",
    "percentageChangeF1Score = 100*(accuracy - metrics.iloc[indexMetrics]['F1 Score'])/metrics.iloc[indexMetrics]['F1 Score']\n",
    "\n",
    "if not (modelStr in metricsOptimalModel['Classifier'].unique()):\n",
    "    metricsOptimalModel.loc[len(metricsOptimalModel)] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]\n",
    "else:\n",
    "    indexMetricsOptimalModel = metricsOptimalModel[metricsOptimalModel['Classifier']==modelStr].index.to_list()[0] \n",
    "    metricsOptimalModel.loc[indexMetricsOptimalModel] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">Multi Layer Perceptron</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelStr = 'Multi Layer Perceptron'\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('mlp', MLPClassifier())])\n",
    "\n",
    "param_grid = {\"mlp__activation\" : ['identity', 'logistic', 'tanh', 'relu'], \"mlp__solver\": ['lbfgs', 'sgd', 'adam'], \"mlp__learning_rate\": ['constant', 'invscaling', 'adaptive'],\\\n",
    "    \"mlp__momentum\": list(np.arange(0,1.1,0.1))}\n",
    "search = HalvingGridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(Xtrain, ytrain)\n",
    "pd.DataFrame(search.cv_results_)\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('mlp', search.best_estimator_['mlp'])])\n",
    "startTime = time.time()\n",
    "pipe.fit(Xtrain,ytrain)\n",
    "stopTime = time.time()\n",
    "trainingTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "stopTime = time.time()\n",
    "\n",
    "accuracyTestTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "f1Score = f1_score(y_test, pipe.predict(X_test), average='macro')\n",
    "stopTime = time.time()\n",
    "\n",
    "f1ScoreTestTime = stopTime - startTime\n",
    "testTime = (accuracyTestTime + f1ScoreTestTime)/2\n",
    "\n",
    "indexMetrics = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "percentageChangeMeanAccuracy = 100*(accuracy - metrics.iloc[indexMetrics]['Mean Accuracy'])/metrics.iloc[indexMetrics]['Mean Accuracy']\n",
    "percentageChangeF1Score = 100*(accuracy - metrics.iloc[indexMetrics]['F1 Score'])/metrics.iloc[indexMetrics]['F1 Score']\n",
    "\n",
    "if not (modelStr in metricsOptimalModel['Classifier'].unique()):\n",
    "    metricsOptimalModel.loc[len(metricsOptimalModel)] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]\n",
    "else:\n",
    "    indexMetricsOptimalModel = metricsOptimalModel[metricsOptimalModel['Classifier']==modelStr].index.to_list()[0] \n",
    "    metricsOptimalModel.loc[indexMetricsOptimalModel] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#69140e\">SVM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelStr = 'SVM'\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('svm', SVC(gamma='auto'))])\n",
    "\n",
    "param_grid = {\"svm__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'], \"svm__degree\": list(np.arange(1,10,1))}\n",
    "search = HalvingGridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(Xtrain, ytrain)\n",
    "pd.DataFrame(search.cv_results_)\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('svm', search.best_estimator_['svm'])])\n",
    "startTime = time.time()\n",
    "pipe.fit(Xtrain,ytrain)\n",
    "stopTime = time.time()\n",
    "trainingTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "stopTime = time.time()\n",
    "\n",
    "accuracyTestTime = stopTime - startTime\n",
    "\n",
    "startTime = time.time()\n",
    "f1Score = f1_score(y_test, pipe.predict(X_test), average='macro')\n",
    "stopTime = time.time()\n",
    "\n",
    "f1ScoreTestTime = stopTime - startTime\n",
    "testTime = (accuracyTestTime + f1ScoreTestTime)/2\n",
    "\n",
    "indexMetrics = metrics[metrics['Classifier']==modelStr].index.to_list()[0] \n",
    "percentageChangeMeanAccuracy = 100*(accuracy - metrics.iloc[indexMetrics]['Mean Accuracy'])/metrics.iloc[indexMetrics]['Mean Accuracy']\n",
    "percentageChangeF1Score = 100*(accuracy - metrics.iloc[indexMetrics]['F1 Score'])/metrics.iloc[indexMetrics]['F1 Score']\n",
    "\n",
    "if not (modelStr in metricsOptimalModel['Classifier'].unique()):\n",
    "    metricsOptimalModel.loc[len(metricsOptimalModel)] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]\n",
    "else:\n",
    "    indexMetricsOptimalModel = metricsOptimalModel[metricsOptimalModel['Classifier']==modelStr].index.to_list()[0] \n",
    "    metricsOptimalModel.loc[indexMetricsOptimalModel] = [modelStr, accuracy, f1Score, trainingTime, testTime, percentageChangeMeanAccuracy, percentageChangeF1Score]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<span style=\"color:#3c1518\">Results</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsOptimalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figAccScores = px.bar(y = metricsOptimalModel['Accuracy'],x = metricsOptimalModel['Classifier'], title='Accuracy',width=1000)\n",
    "figAccScores.update_layout(\n",
    "    title=\"Accuracy diagram\",\n",
    "    xaxis_title=\"Classifier\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\") \n",
    ")\n",
    "figAccScores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figAccScores.show()    \n",
    "\n",
    "figF1Scores = px.bar(y = metricsOptimalModel['F1 Score'],x = metricsOptimalModel['Classifier'], title='F1 Scores',width=1000)\n",
    "figF1Scores.update_layout(\n",
    "    title=\"F1 Scores diagram\",\n",
    "    xaxis_title=\"Classifier\",\n",
    "    yaxis_title=\"F1 Score\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\") \n",
    ")\n",
    "figF1Scores.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figF1Scores.show()     \n",
    "\n",
    "figPercentageChangeAcc = px.bar(y = metricsOptimalModel['Percentage Change in Accuracy'],x = metricsOptimalModel['Classifier'],\\\n",
    "     title='Percentage Change in Accuracy',width=1000)\n",
    "figPercentageChangeAcc.update_layout(\n",
    "    title=\"Percentage Change in Accuracy diagram\",\n",
    "    xaxis_title=\"Classifier\",\n",
    "    yaxis_title=\"Percentage Change in Accuracy\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\") \n",
    ")\n",
    "figPercentageChangeAcc.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figPercentageChangeAcc.show()     \n",
    "\n",
    "figPercentageChangeF1 = px.bar(y = metricsOptimalModel['Percentage Change in F1 Score'],x = metricsOptimalModel['Classifier'],\\\n",
    "     title='Percentage Change in Accuracy',width=1000)\n",
    "figPercentageChangeF1.update_layout(\n",
    "    title=\"Percentage Change in F1 Score diagram\",\n",
    "    xaxis_title=\"Classifier\",\n",
    "    yaxis_title=\"Percentage Change in F1 Score\",\n",
    "    font=dict(\n",
    "    family=\"Verdana, monospace\",\n",
    "    size=18,\n",
    "    color=\"#3c1518\") \n",
    ")\n",
    "figPercentageChangeF1.update_traces(marker_color='#a44200', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.0)\n",
    "figPercentageChangeF1.show()     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "846e9557d80d190f9a7f4ea8a91daae54d434e9bf57f1dd575644ba6159713e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
